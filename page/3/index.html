<!DOCTYPE html><html><head>
  <meta charset="utf-8">
  
  
  <title>Daniel Cai</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Currently in Megvii Research">
<meta property="og:type" content="website">
<meta property="og:title" content="Daniel Cai">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="Daniel Cai">
<meta property="og:description" content="Currently in Megvii Research">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Daniel Cai" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Daniel Cai</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Share my paper reading about AIGC &amp; Multimodal &amp; LLM</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"></button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Details or Artifacts A Locally Discriminative Lear ed4c725bf8e142738f7156c4dbaf1b5a" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/04/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/" class="article-date">
  <time class="dt-published" datetime="2022-05-04T02:23:04.000Z" itemprop="datePublished">2022-05-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/04/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/">Details or Artifacts A Locally Discriminative Learning Approach to</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Realistic Image Super-Resolution</p>
<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>GAN-based 的方法在生成details的同时也会引入artifacts。作者发现在artifacts area的 residue variance 和 visual-friendly 的区域有显著的不同，设计上利用生成的artifacts map 来对训练过程进行正则。</p>
<p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled.png" alt="Untitled"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="GAN-SR-induced-visual-artifacts"><a href="#GAN-SR-induced-visual-artifacts" class="headerlink" title="GAN-SR induced visual artifacts"></a>GAN-SR induced visual artifacts</h3><p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>Reconstruct loss 使得结果趋于blurred average， Adv loss 会产生更多细节。由于从一个blur image 开始，SR存在多组解。GAN loss 会把结果引入多个方向。</li>
</ul>
<p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>对于type B 的texture，由于在小区域内的分布是相对随机的，因此很难看出差别</li>
<li>对于type C 的texture，包含 regular &amp; sharp transitions, 在降质后这些pattern 在LR patch 中就消失了。采用原先的GAN方法，它在训练当中的variation 是相当高的</li>
</ul>
<p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled%203.png" alt="Untitled"></p>
<h3 id="Discriminating-artifacts-from-realistic-details"><a href="#Discriminating-artifacts-from-realistic-details" class="headerlink" title="Discriminating artifacts from realistic details"></a>Discriminating artifacts from realistic details</h3><p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled%204.png" alt="Untitled"></p>
<ul>
<li>生成一张artifacts map, 它的通道数为1. 首先将SR的结果和GT做差，提取出high frequency的部分</li>
<li>type B / C 的 Residue 图赏可以看出，他们的差异很大。type B 的Residue 分布依然比较随机，本文采用如下方式计算Residue的方差。经验上把这个窗口的大小设为7.</li>
</ul>
<p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled%205.png" alt="Untitled"></p>
<ul>
<li>为了使得训练过程更加稳定，用指数对方差进行scale 上的调整</li>
<li>用moving average方法来使的训练过程更稳定。该方法有两个输出，其中用EMA方法输出的结果更稳定，用原SR结果生成的细节则更多。当R1 &gt; R2时，则用Refinment map 对结果进行惩罚。</li>
</ul>
<p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled%206.png" alt="Untitled"></p>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled%207.png" alt="Untitled"></p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/Untitled%208.png" alt="Untitled"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/04/Details%20or%20Artifacts%20A%20Locally%20Discriminative%20Lear%20ed4c725bf8e142738f7156c4dbaf1b5a/" data-id="clktabfn00007fbree2z49v94" data-title="Details or Artifacts A Locally Discriminative Learning Approach to" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-ClassSR A General Framework to Accelerate Super-Re 7b62788ea6354524a183401ca1a57572" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/04/04/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/" class="article-date">
  <time class="dt-published" datetime="2022-04-04T02:23:04.000Z" itemprop="datePublished">2022-04-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/04/04/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/">ClassSR A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="ClassSR-A-General-Framework-to-Accelerate-Super-Resolution-Networks-by-Data-Characteristic"><a href="#ClassSR-A-General-Framework-to-Accelerate-Super-Resolution-Networks-by-Data-Characteristic" class="headerlink" title="ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic"></a>ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>核心加速方案是：不同的图像区域进行恢复的难度不同，可以用不同capacity的网络结构来处理。提出一种新的pipeline，ClassSR，即先将图像划分为不同大小的sub images，再根据restoration的难度做分类，再做SR。</p>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled.png" alt="Untitled"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li>作者首先进行了一组观察，问了获取32X32 LR图像的统计特性，先将sub image 过一个网络（MSRResNet）, 再根据各自的PSNR值做一个排序。如图所示，分为三档难度，simple, medium, hard。 使用3种的网络来处理不同种类的sub-images, 3种网络的区别主要在于first / last conv layer的通道数量不一致。分别为16， 36 ， 56.</li>
</ul>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>Class Module 通过low-level feature来分辨sub image 是否容易恢复。结构上 5 conv + 1 avg pooling + 1 fc</li>
<li>SR-Module: 包含多个独立的分支，它的结构可以是任何想被加速的对象。本文为了简化，仅用通道数量来表达网络的复杂程度。</li>
</ul>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>Classification Method: 在训练阶段，input sub image会经过所有的SR分支，从而保证Class Module 能够接受从不同SR分支返回到提督。最终生成的结果如下：</li>
</ul>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%203.png" alt="Untitled"></p>
<ul>
<li>Loss：Class loss, 不同难度分类之间的置信度。Average Loss 是为了避免网络总是选择最复杂的branch，这样 Class Module 就会失效。</li>
</ul>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%204.png" alt="Untitled"></p>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%205.png" alt="Untitled"></p>
<ul>
<li>训练总共分为三个阶段：pretrain SR-Module; Fix SR训练分类分支。 最后同时finetune两个部分。</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%206.png" alt="Untitled"></p>
<ul>
<li>图像越大，加速比越大。到8K时加速比接近50%</li>
</ul>
<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><ul>
<li>如果没有Class Loss，则难以收敛。原因是会随机选择三个分支中的一个，导致训练不稳定。</li>
</ul>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%207.png" alt="Untitled"></p>
<ul>
<li>Average Loss 的引入可以带来Flops的下降，能够在两者之间做好权衡。</li>
</ul>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%208.png" alt="Untitled"></p>
<ul>
<li>采用减少channel数量来减少flops，不减少middle层数量的原因是，这部分计算量占比很小</li>
<li>在NR上不太省算力，原因是没有足够多的simple sub images</li>
</ul>
<p><img src="/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/Untitled%209.png" alt="Untitled"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/04/ClassSR%20A%20General%20Framework%20to%20Accelerate%20Super-Re%207b62788ea6354524a183401ca1a57572/" data-id="clktabfmy0004fbre4r56cgao" data-title="ClassSR A General Framework to Accelerate Super-Resolution Networks by Data Characteristic" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-RepVGG Making VGG-style ConvNets Great Again 56f64e1f74a94655a703787829633922" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/03/28/RepVGG%20Making%20VGG-style%20ConvNets%20Great%20Again%2056f64e1f74a94655a703787829633922/" class="article-date">
  <time class="dt-published" datetime="2022-03-28T02:23:04.000Z" itemprop="datePublished">2022-03-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/03/28/RepVGG%20Making%20VGG-style%20ConvNets%20Great%20Again%2056f64e1f74a94655a703787829633922/">RepVGG Making VGG-style ConvNets Great Again</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="TL；DR"><a href="#TL；DR" class="headerlink" title="TL；DR"></a>TL；DR</h2><p>本文提出了一个结构简单但功能强大的卷积神经网络架构，该结构在推理时候具有类似于VGG的backbone，仅由3 x 3 conv和ReLU堆叠组成。但在训练时，它却有着多分支的拓扑结构，它通过重参数化技术实现了训练和推理方面的解耦，因此该模型被称作Rep-VGG. RepVGG 在Image Net 上的top-1 准确率超过了80%，在NVIDIA 1080Ti 上推理速度更快，远快于ResNet-50 / 100 。与新模型相比，也展现了比较好的速度 / 准确率的trade-off</p>
<p><img src="/RepVGG%20Making%20VGG-style%20ConvNets%20Great%20Again%2056f64e1f74a94655a703787829633922/Untitled.png" alt="Untitled"></p>
<p><img src="/RepVGG%20Making%20VGG-style%20ConvNets%20Great%20Again%2056f64e1f74a94655a703787829633922/Untitled%201.png" alt="Untitled"></p>
<p>上图展示的是训练和测试如何实现解耦，中间部分为训练中网络结构，吸收了ResNet 的优点</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Simple-is-Fast-Memory-economical-Flexible"><a href="#Simple-is-Fast-Memory-economical-Flexible" class="headerlink" title="Simple is Fast, Memory-economical, Flexible"></a>Simple is Fast, Memory-economical, Flexible</h3><ul>
<li>有两个因素会导致Flops和速度之间的Gap: MAC 和 并行度。在相同的FLOPs下，具有高并行度的模型可能比具有低并行度的模型快得多。堆叠结构拥有更高的并行度。</li>
</ul>
<p><img src="/RepVGG%20Making%20VGG-style%20ConvNets%20Great%20Again%2056f64e1f74a94655a703787829633922/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>多分支拓扑结构由于需要保留每个分支的结果，直到相加或串联为止，分支中的特征图尺寸保持不变时，需要两倍的内存。</li>
<li>在简单的堆叠结构中，允许计算完改层后立即释放该层的输入所占用的内存。因此这种计算方式也对硬件更加友好。</li>
<li>多分支结构也会造成剪枝上的困难</li>
</ul>
<h2 id="Re-param-for-Plain-Inference-time-Model"><a href="#Re-param-for-Plain-Inference-time-Model" class="headerlink" title=".Re-param for Plain Inference-time Model"></a>.Re-param for Plain Inference-time Model</h2><p><img src="/RepVGG%20Making%20VGG-style%20ConvNets%20Great%20Again%2056f64e1f74a94655a703787829633922/Untitled%203.png" alt="Untitled"></p>
<ul>
<li>如果大kernel 卷积和小kernel 卷积的步长相同，那么就可以用简单叠加的方法合多个分支的卷积核，用单个卷积来替代，小kernel卷积通过补零的方式padding成大卷积的尺寸</li>
<li>对于3x3的图层，将输入填充一个像素，则1 x 1的图层应具有padding = 0</li>
<li>总结起来一共两步变化：把所有卷积变化成3X3，紧接着把多分支合并成一个。</li>
<li>对BN参数进行融合</li>
</ul>
<p><img src="/RepVGG%20Making%20VGG-style%20ConvNets%20Great%20Again%2056f64e1f74a94655a703787829633922/Untitled%204.png" alt="Untitled"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/03/28/RepVGG%20Making%20VGG-style%20ConvNets%20Great%20Again%2056f64e1f74a94655a703787829633922/" data-id="clktabfn5000ffbre6cr8d7mj" data-title="RepVGG Making VGG-style ConvNets Great Again" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Attentive Fine-Grained Structured Sparsity for Ima d881abc7a35646ffaddfd1f15c94b51b" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/02/03/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/" class="article-date">
  <time class="dt-published" datetime="2022-02-03T02:23:04.000Z" itemprop="datePublished">2022-02-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/02/03/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/">Attentive Fine-Grained Structured Sparsity for Image Restoration</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Attentive-Fine-Grained-Structured-Sparsity-for-Image-Restoration"><a href="#Attentive-Fine-Grained-Structured-Sparsity-for-Image-Restoration" class="headerlink" title="Attentive Fine-Grained Structured Sparsity for Image Restoration"></a>Attentive Fine-Grained Structured Sparsity for Image Restoration</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>N:M structured pruning 是目前比较有效的模型压缩技术，本文提出了对每一层实现不同的sturtured sparsity的pruning方法，进而实现准确性和效率之间的tradeoff</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/Untitled.png" alt="Untitled"></p>
<ul>
<li><p>一个满足N：M sparsity的tensor应该满足如下性质。（1）input channel 可以被M整除 （2）每组M个连续weights至少有N个非零权重。weight，input tensor的压缩方法如图（a）所示。两者都能有N：M的压缩空间</p>
</li>
<li><p>可导的N：M Sparsity Search</p>
<ul>
<li>首先将weight表示为M组 1:M sparisity的稀疏weight之和。每一个weight的重要性都用 一个强度参数来衡量</li>
<li>b是一个二值参数，表示该组参数是否被保留还是丢弃，梯度下降中通过STE来优化这个值. B 中p表示weight的优先级分数，用于决定pruning ratio</li>
</ul>
<p>  <img src="/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/Untitled%201.png" alt="Untitled">
  </p>
</li>
<li><p>Priority - Ordered Pruning</p>
<ul>
<li>上述一共有两种度量方式来决定pruning ratio。当两者产生mis alignment时，容易造成性能下降。通过使得Pi+i &lt; Pi, 优先移除强度参数小的权重</li>
</ul>
<p>  <img src="/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/Untitled%202.png" alt="Untitled"></p>
<p>  <img src="/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/Untitled%203.png" alt="Untitled">
  </p>
</li>
<li><p><strong>Loss Function</strong></p>
</li>
</ul>
<p>pruned Loss 表示压缩后模型的MACS，通过如下形式实现算法性能和模型大小之间的tradeoff</p>
<p><img src="/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/Untitled%204.png" alt="Untitled"></p>
<ul>
<li><p>Adaptive Inference</p>
<ul>
<li>通过图像patch的难度，来自适应使用剪枝模型。为了量化图像块的恢复难度，假设图像块越难恢复，GT与恢复结果之间的误差越大。由于在推理时无法获得GT，我们使用了一种轻型卷积神经网络，可以估计GT和目标模型恢复结果之间的均方误差（MSE）。给定由具有不同目标计算预算的SLS训练的多个模型，来给候选模型来打分。</li>
</ul>
<p>  <img src="/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/Untitled%205.png" alt="Untitled"></p>
<p>  <img src="/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/Untitled%206.png" alt="Untitled"></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/02/03/Attentive%20Fine-Grained%20Structured%20Sparsity%20for%20Ima%20d881abc7a35646ffaddfd1f15c94b51b/" data-id="clktabfmv0001fbre0l8ne8al" data-title="Attentive Fine-Grained Structured Sparsity for Image Restoration" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Accelerating Video Object Segmentation with Compre b5e88a9f118147648a5c28ad61b5ff27" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/01/02/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/" class="article-date">
  <time class="dt-published" datetime="2022-01-02T02:23:04.000Z" itemprop="datePublished">2022-01-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/01/02/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/">Accelerating Video Object Segmentation with Compressed Video</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Accelerating-Video-Object-Segmentation-with-Compressed-Video"><a href="#Accelerating-Video-Object-Segmentation-with-Compressed-Video" class="headerlink" title="Accelerating Video Object Segmentation with Compressed Video"></a>Accelerating Video Object Segmentation with Compressed Video</h1><h2 id="TL；DR"><a href="#TL；DR" class="headerlink" title="TL；DR"></a>TL；DR</h2><p>提供了一种高效的，即插即用的加速框架，用于解决半监督点视频物体分割任务。如何加速，利用视频序列的冗余程度以及压缩比特流。为了实现把 关键帧的mask 单向/双向传播给其他帧。另外还设计了residual-based correction module 来fix 错误的mask。</p>
<ul>
<li>Code: <a target="_blank" rel="noopener" href="https://github.com/kai422/CoVOS">https://github.com/kai422/CoVOS</a></li>
</ul>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><ul>
<li>HEVC coding structure包括一系列帧称为（GOP），每个GOP使用三种帧类型：I-frame / P-frame / B-frame. I frame表示完全独立地被编码。P-frame / B-frame 则表示通过来自其他帧的运动补偿和残差来编码。P / B frame 存储的motion vector可以被认为是block-wise 的光流。</li>
<li>Motion compensation in compressed video<ul>
<li>预测motion vector 的模块被称为PU。Prediction Unit,size 可以是64X64， 8X4， 4X4.</li>
<li>PU可以是单向的，也可以双向。P-frame 只包含单向的PU，B-frame 包括双向PU</li>
<li>通过双向的motion vector，可以通过线性组合来重构帧。</li>
</ul>
</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled.png" alt="Untitled"></p>
<ul>
<li>在一些比较旧的编码设置中，例如CVOS，reference 帧的选择也有要求，必须是I-frame。现代Codec方法，允许P-frame，B-frame类型，从其他的 P / B中去获取参考的pixel . 由于Motion Vector比较粗糙，因此也沿用了一个 Residue对恢复的图像进行pixel detail的修复</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>E_i 应当是稀疏的，E_i 的稀疏程度和PU的准确程度成正比</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%202.png" alt="Untitled"></p>
<h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><ul>
<li>把从长度为T的压缩比特流 中decoded sequence记作</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%203.png" alt="Untitled"></p>
<ul>
<li>为了区分，用下标 i / k 来区分关键帧和非关键帧。对于非关键帧的，需要利用光流去做一次warp</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%204.png" alt="Untitled"></p>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%205.png" alt="Untitled"></p>
<p>4.1 <strong>Soft motion-vector propagation module</strong></p>
<ul>
<li>介绍了如何用motion vector来替代光流。用P和V分别来表示segmentation 和 feature</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%206.png" alt="Untitled"></p>
<ul>
<li>前两个代表单向的propagation, 第三个表示双向的，前向和后向分别是等权重的。w表示重构目标中的tuning 参数。u,t为小数时，则对reference帧采用 最近邻 / 双线性插值。</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%207.png" alt="Untitled"></p>
<ul>
<li>为了消除noise / error的影响，用一个decoder 来实现soft 的 propagation。decoder是一个轻量级，对原始的mask进行denoise（参考image的low-level feature）</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%208.png" alt="Untitled"></p>
<ul>
<li>定义一个相似度来衡量propagate 前后的feature</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%209.png" alt="Untitled"></p>
<p>Residual-based correction module</p>
<hr>
<ul>
<li>通过patch generation 和 label matching来建模correction。 先把residue 转换成灰度空间，接着利用二值化得到binary mask。 把Residue 和 dilate后的前景mask 取交集，得到修正后的mask</li>
</ul>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%2010.png" alt="Untitled"></p>
<p><img src="/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/Untitled%2011.png" alt="Untitled"></p>
<h3 id="Key-frame-base-network-selection"><a href="#Key-frame-base-network-selection" class="headerlink" title="Key frame &amp; base network selection"></a>Key frame &amp; base network selection</h3><ul>
<li>根据压缩类型来选择关键帧。关键帧不仅包括 I frame ， 还包括 P frame。因为I frame的数量仅占到5%左右。P帧作为关键帧也可以提升精度，因为motion vector 在P帧中是严格单向的</li>
<li>采用memory network ，例如STM, MiVOS, and STCN比较适合加速</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/01/02/Accelerating%20Video%20Object%20Segmentation%20with%20Compre%20b5e88a9f118147648a5c28ad61b5ff27/" data-id="clktabfmp0000fbree0azbafp" data-title="Accelerating Video Object Segmentation with Compressed Video" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">« Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/04/BLIP2/">BLIP-2 Bootstrapping Language-Image Pre-trainingwith Frozen Image Encoders and Large Language Models</a>
          </li>
        
          <li>
            <a href="/2023/07/02/BLIP/">BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>
          </li>
        
          <li>
            <a href="/2023/07/01/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/">SDXL</a>
          </li>
        
          <li>
            <a href="/2023/06/25/Segment%20Anything/">SAM</a>
          </li>
        
          <li>
            <a href="/2023/05/17/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/">ViLT-Vision-and-Language Transformer</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      © 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>

<script type="text/javascript" charset="utf-8" src="/js/lazyload-plugin/lazyload.intersectionObserver.min.js"></script></body></html>