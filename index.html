<!DOCTYPE html><html><head>
  <meta charset="utf-8">
  
  
  <title>Daniel Cai</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Currently in Megvii Research">
<meta property="og:type" content="website">
<meta property="og:title" content="Daniel Cai">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Daniel Cai">
<meta property="og:description" content="Currently in Megvii Research">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Daniel Cai" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Daniel Cai</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Share my paper reading about AIGC &amp; Multimodal &amp; LLM</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"></button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-BLIP2" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/BLIP2/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T02:23:04.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/BLIP2/">BLIP-2 Bootstrapping Language-Image Pre-trainingwith Frozen Image Encoders and Large Language Models</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><hr>
<p>随着模型计算量的增加，Vision-Language 预训练的代价越来越高。本文提出一种高效的VL预训练策略Blip2，可以利用已经训练好的图像模型和语言模型为基础，在此基础上进行新的预训练，从而获得VL的联合表征。</p>
<ul>
<li>提出一种轻量级的Query Transformer, 该模块可以减少视觉和语言之间的gap，该模块通过两阶段预训练构成</li>
<li>第一阶段从 frozen的图像编码器中引导 Visual language 的学习。第二阶段从frozen的 language model 中引导vision to language 的生成学习。BLIP2在较少的训练参数情况下，取得了SOTA</li>
</ul>
<p><img src="/image/image_3oU_ZEdbGI.png"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><hr>
<p><img src="/image/image_-KS310Em4i.png"></p>
<ul>
<li>Model Arch<ul>
<li>Left: 表示Q-Former 和 BLIP-2 的一阶段 VL联合表征，这里用上了三个目标函数来使得根据query来提取视觉特征。Right：自注意力掩膜策略，来控制query-text interaction。“查询-文本交互”指的是通过设计查询向量与文本特征的自注意力交互方式,来引导查询向量学习表示与特定文本语义相关的视觉概念。</li>
<li>Q-Former从图像编码器中提取固定数量的输出特征,与输入图像分辨率无关。Q-Former 一共包括两个transformer 子模块，他们共享自注意力层的参数。（1）一个是图像transformer,它和冻住的图像编码器互相交互，从而提取视觉特征。（2）一个文字transformer，它既可以作为文字编码器，也可以作为文字解码器。</li>
<li>创建一组可学习的查询嵌入作为图像transformer的输入。查询之间通过自注意力层交互,通过交叉注意力层与冻结图像特征交互(每两个transformer块插入一个)。查询还可以通过同样的自注意力层与文本交互。</li>
<li>对于不同的pretrain任务，使用不同自注意力mask 来实现交互。这里的权重还是用BERT来初始化。</li>
<li>query （查询向量），在实验中，使用32个长度为768的向量作为query。这个参数远小于图像本身（ViT-L / 14）的特征（257*1024）</li>
</ul>
</li>
<li> Bootstrap Vision-Language RepresentationLearning from a Frozen Image Encoder<ul>
<li>将Qformer 和 冻结的image encoder 连接，使用图像-文本对进行预训练，目标是训练Q-former的查询向量，提取和文本最相关的视觉特征。这里会联合优化三个目标，它们们共享相同的输入格式和模型参数。对于每个目标，都使用不同的注意力遮蔽策略来控制「查询向量」和「文本」之间的交互。</li>
<li>ITC：最大化图像表示和文本表示之间的交互信息,通过正负样本对比学习图像-文本的相似度。unimodal self-attention mask 表示单模态的自注意力遮蔽,查询向量和文本之间不允许相互关注。这样可以帮助它们聚焦在各自的单一模态上，不互相干扰。</li>
<li>ITG：给定图像作为条件，训练Q-Former 生成对应的文本。作者的解释是，query(查询向量)提取出文本所需要的视觉特征。这里使用多模态因果自注意力遮蔽来控制查询向量和文本之间的交互。“因果遮蔽”(causal masking)指的是只允许attention模块关注到当前位置之前(左侧)的内容,而不能关注到当前位置之后(右侧)的内容。</li>
<li>ITM：学习图像文本之间细粒度的对齐,判断一个图像文本对是否匹配。查询向量学习多模态信息</li>
</ul>
</li>
</ul>
<p><img src="/image/image_KIEmF1jz9H.png"></p>
<ul>
<li>Bootstrap Vision-language Generative Learning from a frozen LLM<ul>
<li>在生成的预训练阶段，作者把Q-former和LLM连接，来获得LLM的语言生成能力。如图3所示，这里使用FC来调整Q-former 输出向量到LLM的向量维度。</li>
<li>将映射后的查询向量拼接在文本词向量前面,作为软化的视觉提示,为LLM提供相关视觉信息。</li>
<li>Q-Former通过第一阶段的预训练提取语言相关的视觉表示, 作者的解释是这里能起到信息瓶颈的作用,只向LLM提供有用信息,这样避免了视觉特征和文本特征对齐的难度，避免了遗忘问题。</li>
<li>作者对两种LLM做了实验：（1）对于解码器型LLM,使用语言模型损失进行预训练,LLM基于查询向量生成文本。（2）对于编码器-解码器型LLM,使用前缀语言模型损失,将文本拆分成前缀和后缀,前缀与查询向量一起输入编码器,后缀作为解码目标。</li>
</ul>
</li>
<li>Model Pretraining<ul>
<li>预训练数据集:总共1.29亿张图像,来源包括COCO、Visual Genome、CC数据集等。使用BLIP的图像字幕模型为web图像生成字幕,保留top2字幕作为训练数据。</li>
<li>预训练的图像编码器:探索了CLIP的ViT-L/14和EVA-CLIP的ViT-g/14两种视觉Transformer。去掉最后一层,使用倒数第二层作为输出特征。</li>
<li>预训练的语言模型:探索了OPT模型系列的解码器型语言模型,以及FlanT5模型系列的编码器-解码器型语言模型。</li>
<li>预训练设置:第一阶段预训练25万步,第二阶段预训练8万步。ViT-L/ViT-g的批量大小分别为2320和1680,OPT/FlanT5的批量大小分别为1920和1520。使用FP16或Bfloat16量化。</li>
<li>优化器设置:AdamW,波动学习率,线性warmup等。</li>
<li>数据增强:随机裁剪和水平翻转。</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><hr>
<ul>
<li>Instructed Zero-shot Image-to-Text Generation<ul>
<li>这里是把text prompt 加到 visual prompt 之后，展现了很好的泛化能力，<ul>
<li>视觉知识推理:可以正确描述图像中的物体、场景、属性和关系,展示了对视觉知识的理解。</li>
<li>视觉常识推理:可以进行常识推理,描述图像之外不可见的背景信息。</li>
<li>视觉对话:可以进行多轮视觉问答,持续理解图像并生成相关回复。</li>
<li>个性化文本生成:可以结合个性化的语言风格或背景知识生成个性化的文本描述,而不仅是普遍的对象描述</li>
</ul>
</li>
<li>更强的图像编码器或者更大LLM都可以提升性能。这验证了BLIP-2是一个通用的高效预训练方法,可以利用视觉和语言模型的进步。</li>
</ul>
</li>
</ul>
<p><img src="/image/image_SFMVVTssPW.png" alt="我" title="我"></p>
<ul>
<li>Effect of Vision-Language Representation Learning.<ul>
<li>作者说第一阶段Q-former 通过参考文字来学习视觉特征，可以降低在LLM中对齐vision-language的难度。如果没有这个阶段，就只能依赖视觉-文本的生成阶段来降低两种模态之间的gap, 图五表明了两种LLM下，缺乏这个阶段，在VQA任务上都是掉点的。</li>
</ul>
</li>
</ul>
<p><img src="/image/image_CfTA69AJHV.png"></p>
<ul>
<li>Image Captioning<ul>
<li>在图像字幕任务上,使用“a photo of”作为LLM的初始输入,生成完整字幕。只更新Q-Former和图像编码器的参数,LLM保持冻结。BLIP2 在Nocaps zero-shot 任务上取得了更好的结果</li>
</ul>
</li>
</ul>
<p><img src="/image/image_eJudSwIveE.png"></p>
<ul>
<li>VQA<ul>
<li>微调Q-Former和图像编码器的参数,语言模型LLM保持冻结。</li>
<li>使用开放式的答案生成作为损失函数,LLM以Q-Former的输出和问题作为输入,生成答案。</li>
<li>为了提取与问题更相关的图像特征,额外使用问题来条件化Q-Former。</li>
<li>具体来说,问题的词符作为输入给Q-Former,通过自注意力层与查询向量交互,指导交叉注意力层聚焦在更有信息量的图像区域。</li>
</ul>
</li>
</ul>
<p><img src="/image/image_8NZuaqKaOh.png"></p>
<ul>
<li>Image Retrieval<ul>
<li>直接finetune 第一阶段的模型，在COCO数据集上同时微调图像编码器和Q-Former,使用与预训练相同的目标函数,包括ITC、ITM和ITG。ITC和ITM损失函数对学习图像文本相似度非常关键。实验也展示ITG损失对检索 TASK也有帮助,因为它迫使查询向量提取与文本最相关的视觉信息,提升了视觉语言对齐效果。</li>
</ul>
</li>
</ul>
<p> </p>
<p><img src="/image/image_iwAra_JmZL.png"></p>
<p><img src="/image/image_kk34T7H_rh.png"></p>
<h2 id="Thought"><a href="#Thought" class="headerlink" title="Thought"></a>Thought</h2><hr>
<p>在limitation 里作者讨论了BLIP的一些局限性和未来的工作方向。一些现象是说，在VQA任务上，即便提供了上下文，性能也没有提升。可能是因为预训练数据集中每个样本只包含一对图像文本,语言模型无法从中学习多个图像文本对的上下文关系。</p>
<p>使用了冻结模型,BLIP-2也继承了语言模型的风险,如生成攻击性语言、传播社会偏见或泄露私人信息。缓解方法包括使用指令引导生成或在过滤数据集上训练</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/07/04/BLIP2/" data-id="clktabfna000ofbre2s6lao59" data-title="BLIP-2 Bootstrapping Language-Image Pre-trainingwith Frozen Image Encoders and Large Language Models" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-BLIP" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/BLIP/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T02:23:04.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/BLIP/">BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><hr>
<p>VLP（vision-language pretraining），现有的预训练模型在内容理解类任务或者生成类任务上仅能做好两者之一，无法同时兼顾。随着scaling up noisy 的文本-图像对数据集，该任务取得了巨大的进步。BLIP采用 bootstraping caption的方法来利用充满噪声的网络数据，该captioner 会生成合成的字幕并且将带噪的字幕做一次过滤。BLIP方法在图文搜索，图像打字幕以及视觉问答上都取得了巨大的进步。</p>
<p><img src="/image/image_hClQ8eMB6c.png"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><hr>
<p><img src="/image/image_u5FW21qVLU.png"></p>
<ul>
<li>BLIP，预训练模型的结构使用了多模态混合的 encoder-decoder结构，一种固定的VL模型，可以处理三种之一的功能。（1）单模态的编码器通过ITC loss 训练，作用是对齐视觉和文本的表征 （2）Image-grounded 文字编码器使用额外的跨注意力层来建模视频语言的交互，训练中使用image-text matching (ITM loss) 来区分图文对中的正样本和负样本。（3）Image grounded  文本解码器使用 causal self-attention layer 替换了双向的 self-attention layer。 同时和encoder使用了相同参数的cross attention / feed forward 。 解码器使用LM （language modeling loss ）来生成字幕</li>
<li>Model Arch：使用Vit 模型作为Image Encoder。先把image  切分成多块patch，并把其编码成一系列的向量，同时带上额外的「CLS」token 来表示完整的图像特征。<ul>
<li>Unimodal encoder: 文本编码器和视觉编码器是独立的。BERT 的text encoder。同时插入在text input 前插入一个「CLS」token。</li>
<li>Image-grounded text encoder: 在每个text encoder 的transformer 模块中，在self-attention 和 feed-forward 模块中间插入跨注意力层。一个task-specific [encoder] token 被添加到text 当中，它的输出则作为图文对的一种多模态表示</li>
<li>Image-grounded text decoder: 一个「Decode」token被用开始的信号，end-of-sequence token 用于表示结束的信号。</li>
</ul>
</li>
<li>Pretraining Objective<ul>
<li>在预训练阶段，同时优化三个目标函数。其中两个（ITC/ITM）是理解类的目标函数，LM属于生成类的目标函数。从图中可以看出，对于一个图像文本对的样本，它的前向需要经过一个计算量大的VIT网络，同时也要通过三次文本编码器。</li>
<li>ITC：原理是鼓励正样本有相似的相似的表征，而负样本则尽可能的不同。ITC loss 使用了带momentum 的编码器来生成特征。同时使用编码器来生成软标签，从而从负样本中找到正样本</li>
<li>ITM：主要作用在Image-grounded text encoder 上，能够生成图文多模态表征。ITM是二分类任务，模型使用ITM Head（linear）来预测图文是否匹配。为了使得从负样本中挖掘更多信息，使用了负样本挖掘技术，具体来说就是用相似度较高的负样本来计算loss</li>
<li>LM：cross entropy loss 。在计算loss使用了0.1 权重的label smoothing</li>
<li>利用多任务学习的特性，所有文本编码器中的自注意力层参数都是共享的。对于current input token, 文本编码器使用双向注意力层来建立编码表示。对于解码器，使用causal 自注意力层来预测下一个token。</li>
</ul>
</li>
<li>CapFilt<ul>
<li>由于标注成本的问题，高质量的标注图文数据记为${(I_h, T_h)}$，典型数据就是COCO。和网络上搜集到巨量文本对，${(I_w, T_w)}$。主要包括：一个输入web 数据, 生成caption的captioner .一个能够筛选过滤 noisy 数据的filter。这两部分的模型权重都由MED预训练权重得到。</li>
<li>captioner 的结构和image -grounded text decoder 一致，使用LM目标函数来进行finetune. 对 $I_w$ 进行打字幕得到文本合成描述 $T_s$。用ITC ITM来训练fliter， 过滤器过滤掉了$T_s, T_w$。最终得到一个新的完整数据集<br><img src="/image/image_DpMYDs5kjC.png"></li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><hr>
<ul>
<li><p>Pretraining Details</p>
<ul>
<li>图像编码器的预训练模型是在Imagenet上训练得到的，文本编码器的预训练模型由BERT来初始化。本文探索了两种配置 Vit-B / Vit-L. pretrain 时用了20个epoch，其中2880 bs for Vit-B / 2400 for Vit-L.  Learning rate 采用warmup的策略，3e-4/2e-4 for Vit-B/ Vit-L . pretrain 时候使用224x224的图像，finetune时候使用384. 训练数据如下图所示：<br><img src="/image/image_5_sS0UauI7.png"></li>
</ul>
</li>
<li><p>Effect of Capfilt</p>
<p><img src="/image/image_hr9EFyi4P9.png"></p>
<ul>
<li>作者在两个不同量级的数据集进行了比较，在检索 / caption 任务上都证明了有效性。当caption 和filter相互搭配之后，效果能够进一步提升。并且随着scaling up 数据集和模型大小，都能继续涨点。</li>
<li>图中可以看出，captioner 以及 filter的作用<br><img src="/image/image_Lgcw91ZpOW.png"></li>
</ul>
<p><img src="/image/image_CjVPNUu-V1.png"></p>
<ul>
<li><p>diversity is key for synthetic captions</p>
<ul>
<li>使用nucleus sampling 来生成合成的字幕，作者的解释是因为产生了更加多样性的caption ，尽管noise ratio更高，也能在检索，caption任务上涨点。相比之下beam 生成的caption更加普通，就难以从中获取更多的知识</li>
</ul>
</li>
<li><p>Parameter Sharing and Decoupling</p>
<ul>
<li>除了self-attention 模块，对所有参数进行共享能够取得最好的效果。作者的解释是，如果self-attention 也是共享的，encoder decoder的冲突将会导致性能的下降</li>
</ul>
<p><img src="/image/image_5mV9YGxTZf.png"></p>
<ul>
<li>在CapFilt中，对图像标注生成器（captioner）和过滤器（filter）进行端到端的独立微调（finetune）任务，任务是在COCO数据集上进行。参数共享：在Table 4中，研究了在captioner和filter之间共享参数的影响，共享方式与预训练相同。结果显示，共享参数导致下游任务性能下降，主要原因被归因于“confirmation bias”（确认偏差）。确认偏差：由于参数共享，captioner产生的带有噪声的标注不太可能被filter过滤掉，这导致captioner生成的低质量标注在下游任务中被误认为是正确的，从而降低了性能。这也可由更低的噪声比率（8%相比于25%）来表示，即captioner生成的噪声标注在参数共享下被较少过滤掉。<br><img src="/image/image_gru0jHQV9E.png"></li>
<li>在COCO 数据集上，对TR / IR任务进行推理和测试。为了提高推理速度，作者遵循了Li等人（2021a）的方法，首先基于图像与文本特征的相似性选择了k个候选项，然后根据候选项之间的配对ITM得分重新排序。对于COCO数据集，设置k=256，对于Flickr30K数据集，设置k=128。使用14M的pretrain 模型，比先前的ALBEF高了不少。</li>
</ul>
<p><img src="/image/image_rpnfoabuKC.png"></p>
<ul>
<li>测试了一下zero shot,  即在COCO上finetune, 再放到Flicker 30K 上面去测试也能获得不错的效果</li>
</ul>
</li>
<li><p>Image Captioning </p>
<ul>
<li>每张caption前面都加上  a picture of ，可以提升效果。14M 的BLIP再相同量级的预训练数量上比其他方法要好。LEMON还使用了一个更大的检测器以及更高的分辨率，CLIP不带检测器而且效果也更好</li>
</ul>
</li>
</ul>
<p><img src="/image/image_dKzMOuAH9D.png"></p>
<ul>
<li>VQA<ul>
<li><p>下游任务的模型结构如图所示</p>
<p><img src="/image/image_xQqnnR3Dio.png"></p>
<p><img src="/image/image_2ruSGh-p58.png"></p>
<p><img src="/image/image_KGtQHk4vkh.png"></p>
<p><img src="/image/image_lESF2m1osP.png"></p>
<p><img src="/image/image_qvNqtLyTHE.png"></p>
</li>
<li><p>作者还做了一些对比实验，证明了bootstrap 效果提升并不是因为训练时间更长导致的。当bootstrap 的数据拿来预训练一个新数据集时，对原来的pretrained 模型继续训练，效果也没有任何提升了。作者解释说，从知识蒸馏的角度上来看，老师模型不能拿来给学生模型做初始化</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Thought"><a href="#Thought" class="headerlink" title="Thought"></a>Thought</h2><hr>
<ul>
<li>多模态任务的预训练，共享了除self attention之外的参数</li>
<li>bootstrap相当于重新获得了一部分高质量的数据</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/07/02/BLIP/" data-id="clktabfmx0003fbre2sfmdoca" data-title="BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-SDXL Improving Latent Diffusion Models for High-Re b0dfed5065764baaa371b5b7fd7275cd" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/01/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/" class="article-date">
  <time class="dt-published" datetime="2023-07-01T02:23:04.000Z" itemprop="datePublished">2023-07-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/01/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/">SDXL</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="SDXL-Improving-Latent-Diffusion-Models-for-High-Resolution-Image-Synthesis"><a href="#SDXL-Improving-Latent-Diffusion-Models-for-High-Resolution-Image-Synthesis" class="headerlink" title="SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"></a>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>SDXL 使用了三倍大小的UNet Backbone，主要体现在：更多的attention block, 更大的cross attention context 作为第二个text encoder。 设计了多种条件机制，并且在SDXL上使用多种aspect ratio 进行训练，最后引入一个refine model 来给SDXL做后处理。</p>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Stability-AI/generative-models">https://github.com/Stability-AI/generative-models</a></p>
<p>Model weights: <a target="_blank" rel="noopener" href="https://huggingface.co/stabilityai/">https://huggingface.co/stabilityai/</a></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/Untitled.png" alt="Untitled"></p>
<ul>
<li>两阶段pipeline: 首先使用SDXL生成初始128 x 128 大小的latent ，其次利用一个专门的高分辨率refinement 模型，和SDEdit 方法作用在第一次输出的latent 上（使用相同的prompt）。SDXL / Refinement model 使用相同的自编码器。</li>
</ul>
<h2 id="Architecture-Scale"><a href="#Architecture-Scale" class="headerlink" title="Architecture &amp; Scale"></a>Architecture &amp; Scale</h2><p><img src="/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>Contrast<ul>
<li>在UNet 参数上，几乎是以前的三倍</li>
<li>在UNet 当中的Transformer 模块的分布做了一些调整，去掉了8X downsample, 也增加了低分辨率上transformer 的数量。这里主要的原因是因为效率</li>
<li>Text Encoder 方面用了OpenCLIP Vit BigG 和 CLIP Vit-L</li>
<li>context dim 的大小也增大了不少</li>
</ul>
</li>
</ul>
<h2 id="Micro-Conditioning"><a href="#Micro-Conditioning" class="headerlink" title="Micro-Conditioning"></a>Micro-Conditioning</h2><ul>
<li><p>Conditioning the Model on Image Size</p>
<ul>
<li>将原始的输入输出size 当作condition 注入。利用傅立叶特征编码对每个部分单独计算嵌入。这些编码最终concat 成一个向量，然后和time embedding 相加。</li>
</ul>
<p>  <img src="/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>512 only 因为存在overfit 现象，效果很差；CIN-size-cond 的FID和IS都很高</li>
</ul>
<p>  <img src="/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/Untitled%203.png" alt="Untitled">
  </p>
</li>
<li><p>Conditioning the Model on Cropping Parameters</p>
<ul>
<li>可以看出SDXL相比之前的版本，Crop问题有更好的表现</li>
<li>再设置一个condition, 一共包含top, left 两个元素。在训练神经网络时，防止随机裁剪造成的数据泄露问题：在数据加载阶段，提出了均匀采样裁剪坐标的方法，将它们通过傅立叶特征嵌入作为条件参数输入模型，从而增强了图像合成过程的控制。</li>
</ul>
<p>  <img src="/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/Untitled%204.png" alt="Untitled"></p>
</li>
</ul>
<p><img src="/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/Untitled%205.png" alt="Untitled"></p>
<ul>
<li><p>Multi-Aspect training</p>
<ol>
<li>分桶训练：将数据分为不同宽高比的桶，每个训练批次由同一桶中的图像组成，并在每个训练步骤中交替使用不同大小的桶。</li>
<li>尺寸条件化：模型接收桶大小（或目标大小）作为条件输入，以整数元组 $c_{ar}$ = ($h_{tgt}, w_{tgt}$  )的形式表示，并以类似于之前描述的尺寸和裁剪条件的方式嵌入到傅立叶空间中。</li>
<li>多方位fintuen：在固定宽高比和分辨率的预训练模型之后，进行多方位训练作为微调阶段，并将其与第2.2节中介绍的条件技术结合，通过沿通道轴连接。</li>
<li>裁剪条件化与多方位训练互补：裁剪条件化和多方位训练是互补的操作，裁剪条件化只在桶边界（通常为64像素）内工作。然而，为了简化实施，选择保留这个控制参数用于多方位模型。</li>
</ol>
</li>
<li><p>Improved Autoencoder</p>
<ul>
<li>用了更大的batchsize</li>
</ul>
</li>
<li><p>Putting everything together</p>
<ul>
<li>在latent 空间使用单独的LDM来处理高频数据</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/07/01/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/" data-id="clktabfn7000ifbregelob453" data-title="SDXL" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Segment Anything" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/25/Segment%20Anything/" class="article-date">
  <time class="dt-published" datetime="2023-06-25T02:23:04.000Z" itemprop="datePublished">2023-06-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/25/Segment%20Anything/">SAM</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Segment-Anything"><a href="#Segment-Anything" class="headerlink" title="Segment Anything"></a>Segment Anything</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><hr>
<p>在图像分割方面定义了一种新的任务，模型（SAM），数据（SA-1B），该项目搜集了11M 图像和 1B 的mask。该模型在很多任务上完全了评估，展示出比较好的zero-shot性能。</p>
<p><img src="/image/image_uEhvx1I4w0.png"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><hr>
<p><img src="/image/image_YADwRIRu3Z.png"></p>
<ul>
<li><strong>Segment Anything Model</strong><ul>
<li>图像编码器：使用MAE预训练的Vision transformer，可以比较好的适配高分辨率的输入。</li>
<li>提示编码器：可以是两种类型的提示：稀疏的（point, box, text）, 密集的mask。position encoding 表示点和框，加上学习到的该类型的嵌入向量；现成的CLIP编码器来提取文本；mask则用Conv来提取，并和图像的embed做逐元素的相加。</li>
<li>Mask解码器: decoder 将 image embed 和 prompt embed 和 输出token映射到一个mask。采用的是一种修改版TF decoder模块，后接一个dynamic mask prediction head. 在提示上使用cross attention, self attention 来更新所有embed。过完上述两个模块之后，对图像嵌入进行上采样，然后使用MLP将输出token映射到分类器，该分类器在每个图像位置计算掩模前景概率。</li>
<li>消除歧义: 对于有歧义的propmt，模型会平均多个有效的mask。本文的做法是对于同一个prompt，输出3个mask已经足够了。</li>
<li>效率: 浏览器CPU上接近50ms,相当于实时的体验</li>
<li>train &amp; loss: focal loss 和 dice loss 的混合。对于几何上的prompt, 采用了多种形式的混合采样来模拟交互式.</li>
</ul>
</li>
<li><strong>Segment Anything Data Engine</strong><ul>
<li>data engine: 其实就是标注工具，文中把它分为三个阶段 ：<ul>
<li>（1）模型辅助下手工标注：这个阶段在SAM的帮助下，每张图的mask 从 22 增加到44. 标注速度仅比 bounding box 慢一半。</li>
<li>（2）半自动，即模型自动生成 和 模型辅助下 的混合. 这个阶段标注时间又回到34s， 但是每张图像的mask增加到72</li>
<li>（3）模型全自动生成 ：比较特别的是，在prompt 方面，用了32x32的点来</li>
</ul>
</li>
</ul>
</li>
<li>Segment Anything Dataset<ul>
<li>Images: 获取了一波11M的图像，将最短边缩短到1500，依然比现有数据集分辨率都高</li>
<li>Mask:1.1B ，mask 质量也很高<br><img src="/image/image_5aHFa97wFs.png"></li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><hr>
<ul>
<li><strong>Zero-Shot Transfer Experiments</strong><ul>
<li><strong>Zero-Shot Single Point Valid Mask Evaluation</strong></li>
<li>edge detection<ul>
<li>对于生成的mask过一个sobel filter，已经能获得质量比较高的edge map</li>
</ul>
</li>
<li>物体检测和实例分割表现也很好</li>
<li>text to mask<ul>
<li>在训练期间，使用提取的 CLIP 图像embed 作为其第一个输入提示 SAM。由于 CLIP 的图像嵌入经过训练以与其文本embed对齐，因此我们可以使用图像embed进行训练，但使用文本embed进行推理。在推理时可以通过 CLIP 的文本编码器运行文本，然后将生成的文本embed作为提示提供给 SAM</li>
</ul>
<hr>
<hr>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/25/Segment%20Anything/" data-id="clktabfn7000jfbrefjx42emi" data-title="SAM" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-ViLT Vision-and-Language Transformer Without Convo ddf0a31cddb9402397b4e99fe718307a" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/17/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/" class="article-date">
  <time class="dt-published" datetime="2023-05-17T02:23:04.000Z" itemprop="datePublished">2023-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/17/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/">ViLT-Vision-and-Language Transformer</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="ViLT-Vision-and-Language-Transformer"><a href="#ViLT-Vision-and-Language-Transformer" class="headerlink" title="ViLT: Vision-and-Language Transformer"></a>ViLT: Vision-and-Language Transformer</h1><p>Without Convolution or Region Supervision</p>
<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL; DR"></a>TL; DR</h2><p>VLP 大幅提升了 joint vision-language 下游任务的效果。现有方法主要依靠image feature extraction 的过程，主要包括ROI 区域的监督和卷积结构。痛点：（1）efficiency / speed ：直接提取图像特征需要额外的计算量（相比多模态交互）（2）表达能力的上界受限于 visual embedder 和 predeﬁned visual vocabulary（预定义好的视觉词汇）</p>
<p><img src="/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/Untitled.png" alt="Untitled"></p>
<p><img src="/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>常见的四种vision language model. VE / TE / MI 分别代表视觉嵌入器 / 文本嵌入器 / 多模交互。其中越高则表示计算量越大</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><ul>
<li>ViLT(视觉语言变化器)采用了简单的视觉嵌入流程和单流方式。它和其他方法的区别是：它从预训练的ViT 而不是BERT 来初始化交互变换器的权重。这种初始化主要是为了应用交互层的视觉特征能力，而无需单独的视觉嵌入器。</li>
<li>Vit 包括 MSA，MLP层，它和BERT比较大区别在于：BERT是 post-norm, 即 layer norm 在MSA，MLP之后，Vit 中的layer norm 则在之前。</li>
<li>输入文本经过 word embed Matrix 和 位置编码矩阵得到词嵌入。输入图像则经过切块，展开之后，经过线性映射和位置编码，最终得到嵌入向量。text 和 image 先沿着各自模态求和，然后concat 得到一个向量z0，这个向量再经过depth transformer 反复进行迭代。p 表示对模态输入的第0维，做一次线性映射然后再取正切。</li>
</ul>
<p><img src="/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>模型使用的weight 来自于 ViT-B/32</li>
</ul>
<h3 id="Pre-training-Objectives"><a href="#Pre-training-Objectives" class="headerlink" title="Pre-training Objectives"></a>Pre-training Objectives</h3><ul>
<li>模型训练ViLT 使用了两个常用的目标<ul>
<li>ITM（Image text matching）图像文本对齐：在处理过程中，随机把原有的对齐图像以0.5的概率替换成另一张不同图像。ITM头部使用一个线性层将pool的输出特征p投影到二元类上，然后计算-log似然作为ITM损失</li>
<li>词片对齐(word region alignment)。WPA使用不精确的近端点方法（Inexact Proximal Point method，IPOT）计算zD的两个子集（文本子集zD|t和视觉子集zD|v）之间的对齐得分</li>
<li>MLM (Masked Language Modeling):  目标是从 $z_{masked}^{D} | t$ 去预测 $t_{mask}$(mask text token). 使用两层MLP，MLM head， 输入 $z_{masked}^{D} | t$ ， 输出词汇表上的logits值，然后计算损失。</li>
</ul>
</li>
</ul>
<h3 id="Whole-word-masking"><a href="#Whole-word-masking" class="headerlink" title="Whole word masking"></a>Whole word masking</h3><ul>
<li>whole word masking 是指把所有连续的sub word 全部屏蔽，这个技术重要的原因是，希望可以充分利用另外模态的信息，避免直接从被部分屏蔽的sub words 中预测出整个词。</li>
</ul>
<h3 id="Image-Augmentation"><a href="#Image-Augmentation" class="headerlink" title="Image Augmentation"></a>Image Augmentation</h3><ul>
<li>finetune 过程中使用了RandAugment， 避免使用颜色反转和crop</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h3><ul>
<li>在以下数据集中进行预训练</li>
</ul>
<p><img src="/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/Untitled%203.png" alt="Untitled"></p>
<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ul>
<li>lr: 1e-4, weight decay: 1e-2. 前10%时进行warm up</li>
<li>保持图像的长宽比不变，短边限制在384，长边限制在640。对于640X384的图像，ViLT-B/32 会产生12 x 20 个patch 。大部分情况patch 数量在200 左右。</li>
<li>训练使用了64块V100，batchsize=4096；</li>
</ul>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p><img src="/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/Untitled%204.png" alt="Untitled"></p>
<ul>
<li>与其他配备了大型视觉嵌入模块的视觉语言预训练模型相比，ViLT 在 VQA 得分上表现得略有不足。这可能是由于对象检测器生成的独立对象表示能够简化 VQA 的训练，因为在 VQA 中，问题通常是针对特定对象的。</li>
<li>Visual Reasoning 上结果还不错</li>
</ul>
<p><img src="/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/Untitled%205.png" alt="Untitled"></p>
<ul>
<li>检索</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/05/17/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/" data-id="clktabfn9000lfbre5d6jfvr6" data-title="ViLT-Vision-and-Language Transformer" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Magic3D High-Resolution Text-to-3D Content Creatio 3124df4e1360440fbd25806433fa5167" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/15/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/" class="article-date">
  <time class="dt-published" datetime="2023-05-15T02:23:04.000Z" itemprop="datePublished">2023-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/05/15/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/">Magic3D-High-Resolution Text-to-3D Content Creation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Magic3D-High-Resolution-Text-to-3D-Content-Creation"><a href="#Magic3D-High-Resolution-Text-to-3D-Content-Creation" class="headerlink" title="Magic3D: High-Resolution Text-to-3D Content Creation"></a>Magic3D: High-Resolution Text-to-3D Content Creation</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>DreamFusion 证明了用预训练的文生图diffusion 模型来优化nerf的能力，能够取得较好的text-3d 的结果。该方法存在两方面缺点（a）优化Nerf 速度慢 （b）Nerf上低分辨率优化，会生成低质量的3D模型。本文提出了两阶段方法。（1）使用低分辨率的diffusion prior来获得一个“粗糙”的3D Model, 同时用3D 稀疏哈希网格来进行加速。 将这个“粗糙”的表示作为初始化，接着优化一个带纹理的3D mesh model, 优化使用了和高分辨率latent diffusion 相互交互的可微分。该方法能够生成高质量3D hash model，在40分钟之内，相比Dreamfusion 方法有两倍加速，同时给用户提供了控制3D合成的方法。</p>
<p><img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled.png" alt="Untitled"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ul>
<li>DreamFusion 模型包括两个关键部分：（1）场景模型（2）与训练好的text2img 的diffusion 模型。（a）场景模型是一个参数函数x = g(θ)，可以在所需的相机姿态下产生图像x。这里，g是选择的体积渲染器，θ是一个坐标为基础的多层感知机(MLP)，代表一个3D体积。(b)扩散模型ϕ配备了一个已学习的去噪函数ϵ ϕ (x t ; y, t)，可以预测给定噪声图像x t、噪声级别t和文本嵌入y时采样的噪声ϵ。它提供了更新θ的梯度方向，使得所有渲染的图像在扩散先验下，都被推向由文本嵌入条件的高概率密度区域。<br>DreamFusion引入了得分蒸馏采样(Score Distillation Sampling, SDS)，这个过程计算梯度：</li>
</ul>
<p>$$</p>
<p>\nabla_{\theta} L_{SDS} \left( {\phi}, g(\theta) \right) = E_{t, \epsilon} \left[ w(t) \left( \epsilon_{\phi} (x_{t} ; y, t) - \epsilon \right)\frac{\partial x}{\partial \theta}  \right]</p>
<p>$$</p>
<p>去噪函数ϵ ϕ通常被另一个使用无分类器指导（classifier-free guidance）的替代，这允许控制文本条件的强度</p>
<ul>
<li>场景模型选择了带有显式阴影模型的Mip-NeRF 360的变种，并且使用ImageGen 作为扩散模型。这种模型只能在64x64分辨率上操作，无法获得高分辨率的纹理；另外，在大型全局MLP上进行体积渲染在计算和内存上都十分昂贵</li>
</ul>
<h3 id="High-Resolution-3D-Generation"><a href="#High-Resolution-3D-Generation" class="headerlink" title="High-Resolution 3D Generation"></a>High-Resolution 3D Generation</h3><p><img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%201.png" alt="Untitled"></p>
<ul>
<li><p>Coarse-to-fine Diffusion</p>
<ul>
<li>第一阶段，使用了和Dream fusion 中类似的diffusion 模型，该diffusion 先验视用来计算场景模型的梯度，梯度更新依赖于低分辨率低rendered images</li>
<li>第二阶段：使用latent diffusion 在512x512上进行推理，这里使用了stable diffusion model。增加的计算时间主要来自于 $\partial x / \partial \theta$ (来自于高分辨率渲染图像的梯度)， $\partial z / \partial x$ （encoder 的梯度）</li>
</ul>
</li>
<li><p>Scene Models</p>
<ul>
<li>粗糙优化阶段 ： 选择了Instant NGP 中的hash grid encoding ，允许使用更低的计算成本表示高频细节。使用hash grid 和两个单层网络，一个预测反照率和密度，一个预测法线。使用来自Instant NGP的基于密度的体素修剪方法和基于八叉树的射线采样和渲染算法</li>
<li>精细优化阶段：使用带纹理的3D网格来作为精调阶段的场景表示。与为神经场进行体积渲染不同，对带纹理的网格进行可微分栅格化渲染可以在非常高的分辨率下有效地执行。使用coarse 阶段的神经场作为网格几何值的初始值。这里使用可变形的四面体网格（$V_t$，T）来表示3D形状，其中$V_t$是网格T中的顶点。每个顶点$v_i$ ∈ $V_t$ ⊂ R 3 包含一个有符号距离场（SDF）值 $s_i$  ∈ R 和一个顶点从其初始规范坐标的变形 ∆$v_i$ ∈ R 3。然后，使用可微分的行走四面体算法从SDF中提取一个表面网格。对于纹理，使用神经颜色场作为体积纹理的representation。</li>
</ul>
</li>
<li><p>Coarse-to-fine Optimzation</p>
<ul>
<li>Neural field optimization: 初始化一个占用率为256^3的网格（空间中哪些部分会被模型占用），模型周期性地更新这个网格，并使用一种技术叫做八叉树来跳过空的部分，以减少需要处理的计算量。</li>
<li>MLP预测法线：不需要通过计算密度差来估计法线，这可以节省大量的计算资源</li>
<li>使用环境映射MLP模拟背景：为了更好地理解模型应该如何放置在3D场景中，建立一个背景模型。这个模型是用一个小的MLP来创建的，它可以预测光线方向对应的颜色。</li>
<li>网格优化：用一种叫做SDF（有符号距离场）的技术来表示的模型，然后使用一个叫做可微分光栅器的工具将我们的模型渲染成高分辨率的图像。再用高分辨率图像重新回去调整模型</li>
<li>增加焦距来展示细节</li>
<li>使用不同的抗锯齿技术来合成前景和背景</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>  <img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>去除掉背景之后，从图中可以看出Magic3D 方法纹理细节多了很多</li>
</ul>
<p>  <img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%203.png" alt="Untitled"></p>
<p>  <img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%204.png" alt="Untitled"></p>
<ul>
<li><p>Controllable 3D generation</p>
<ul>
<li>Personalized text to 3D:  代表性方法是dreambooth， 首先使用一些特定的图像（例如一只猫/ 一只狗的图片）来微调模型，然后在文本提示中加入一个特殊的标识（例如[V]），就可以根据文本提示来生成一个特定的3D模型。</li>
<li>Prompt-based editing through fine-tuning：分为三个步骤：首先，使用一个prompt来训练一个coarse的模型；然后,改变prompt并使用LDM来对模型进行微调，这个步骤可以提供一个好的初始模型；最后，使用修改后的prompt来优化模型。这种方法在保持模型的基本结构的同时，改变模型的纹理或者形状。</li>
</ul>
<p>  <img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%205.png" alt="Untitled"></p>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/05/15/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/" data-id="clktabfn2000afbre1ztr2s5s" data-title="Magic3D-High-Resolution Text-to-3D Content Creation" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Prompt-to-Prompt Image Editing with Cross Attentio ca603140b6e54d76a5249f2ede70466c" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/17/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/" class="article-date">
  <time class="dt-published" datetime="2023-04-17T02:23:04.000Z" itemprop="datePublished">2023-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/04/17/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/">Prompt-to-Prompt Image Editing</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Prompt-to-Prompt-Image-Editing"><a href="#Prompt-to-Prompt-Image-Editing" class="headerlink" title="Prompt-to-Prompt Image Editing"></a>Prompt-to-Prompt Image Editing</h1><p>with Cross Attention Control</p>
<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>将文本驱动的图像合成拓展到文本驱动的图像编辑是很自然的。这个方向面临的挑战是：编辑任务通常是保留未编辑的部分，保留大部分的原始图像。然而在基于文本驱动的模型中，即使是对文本本身进行微小的改动，也会导致截然不同的结果。在本文之前的STOA方法用mask来缓解这个现象，但也忽略了Mask中间的信息。本文追求一种直观的prompt-to-prompt 仅有文本控制的图像编辑框架。经过研究发现 —— cross attention layer是控制图像空间分布和prompt 中每个单词的关键，基于这个观察，作者提出了几个应用程序，通过仅编辑文本提示来监控图像综合。这包括通过替换一个词进行局部编辑，通过添加说明进行全局编辑，甚至精细地控制一个词在图像中的反映程度</p>
<p><img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled.png" alt="Untitled"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>key observation: 生成图像的外观和结构不仅依赖于random seed, 也和 在diffusion过程中图像pixel 和 text embed 之间的关联</li>
</ul>
<p><img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%202.png" alt="Untitled"></p>
<ul>
<li><p>Cross attention in text-conditioned Diffusion Models</p>
<ul>
<li>使用ImageGen —— text-guided 合成模型作为backbone，只在text-to-image的扩散过程进行适应，原有的SR过程保持不变</li>
<li>noisy image $\phi(zt)$ 被映射成 Query matrix  $Q = l_Q(\phi(zt))$. textual embedding 被映射为 key matrix: $K = l_K(\phi(P ))$, value matrix $V = l_v(\phi(P))$。attention mask 如下, mask 用于衡量Q K之间的相似度，为了增加表达能力，并行也使用了multi-head attention.</li>
</ul>
<p>  $$<br>  M = Softmax(\frac{QK^T}{\sqrt{d}})<br>  $$
  </p>
</li>
<li><p><strong>Controlling the Cross-attention</strong></p>
<p>  <img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%203.png" alt="Untitled"></p>
<ul>
<li><p>生成的图像的空间布局和几何形状依赖于跨注意力图。 可以看出注意力mask 随着关键词汇变换的过程，而且在早期，注意力mask 已经具有雏形。由于注意力反映了整体的组成，我们可以将从原始提示 P 的生成中获得的注意力图 M 注入到修改后的提示 P* 的第二次生成中。这允许生成一个编辑后的图像 I*，该图像不仅根据编辑后的提示进行了操控，而且还保留了输入图像 I 的结构。</p>
<ol>
<li>**<code>DM(zt,P,t,s)</code>**：这是一个扩散过程的单步计算，其输出是噪声图像 <strong><code>zt−1</code></strong> 和注意力图 <strong><code>Mt</code></strong> （如果没有使用则省略）。</li>
<li>**<code>DM(zt,P,t,s){M ← M∗}</code>**：这是我们使用额外给定的图 <strong><code>M∗</code></strong> 来覆盖注意力图 <strong><code>M</code></strong> 的扩散步骤，但保持来自提供的提示的值 **<code>V∗</code>**。</li>
<li>**<code>Mt*</code>**：这是使用编辑提示 <strong><code>P*</code></strong> 产生的注意力图。</li>
<li>**<code>Edit(Mt, Mt*, t)</code>**：这是一个通用的编辑函数，它接收原始和编辑图像在生成过程中的第 <strong><code>t</code></strong> 个注意力图作为输入。</li>
</ol>
<p>  <img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%204.png" alt="Untitled"></p>
</li>
</ul>
</li>
<li><p>Edition 函数的定义</p>
<ul>
<li>word swap</li>
</ul>
<p>  <img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%205.png" alt="Untitled"></p>
<ul>
<li>Adding a new phrase 。P =“a castle next to a river” to P∗ =“children drawing of a castle next to a river”</li>
</ul>
<p>  <img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%206.png" alt="Untitled"></p>
<ul>
<li>Attention re-weighting: P = “a fluffy red ball” 例如改变材质的程度</li>
</ul>
<p>  <img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%207.png" alt="Untitled"></p>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>  文章介绍了一些应用，包括</p>
<ul>
<li><strong>Text-Only Localized Editing.</strong></li>
</ul>
<p>  <img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%208.png" alt="Untitled"></p>
<ul>
<li><strong>Global editing.</strong></li>
</ul>
<p>  <img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%209.png" alt="Untitled"></p>
<ul>
<li><strong>Fader Control using Attention Re-weighting.</strong></li>
</ul>
<p>  <img src="/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/Untitled%2010.png" alt="Untitled"></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/04/17/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/" data-id="clktabfn4000efbre3klnf221" data-title="Prompt-to-Prompt Image Editing" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-DreamFusion Text-to-3D using 2D Diffusion c53dbb2cf5b94254b0bd32733b75085b" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/23/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/" class="article-date">
  <time class="dt-published" datetime="2023-03-23T02:23:04.000Z" itemprop="datePublished">2023-03-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/03/23/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/">DreamFusion Text-to-3D using 2D Diffusion</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="DreamFusion-Text-to-3D-using-2D-Diffusion"><a href="#DreamFusion-Text-to-3D-using-2D-Diffusion" class="headerlink" title="DreamFusion: Text-to-3D using 2D Diffusion"></a>DreamFusion: Text-to-3D using 2D Diffusion</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>在Diffusion上训练百万级别的text image pair 帮助text-image 任务取得了巨大的突破。将该方法放在3D合成任务上，面临如下困难：（1）需要大量标注好的3D数据 （2）高效的3D结构 for denoising 3D data. 本文提出一种概率密度蒸馏的loss ，能够将2D diffusion model 作为先验来优化 参数化的image generator。类似DeepDream, 通过梯度下降优化一个随机初始化的Nerf，使得随机角度渲染出的2D图像达到比较低的损失。生成的3D模型，不仅可以从各个角度查看，也可以用任意光照进行重新照明，甚至合成到其他3D环境当中。这种方法无需3D训练数据，也无需修改图像扩散模型，证明了2D Diffusion 作为预训练模型的有效性。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Diffusion-Models-and-Score-distillation-sampling"><a href="#Diffusion-Models-and-Score-distillation-sampling" class="headerlink" title="Diffusion Models and Score distillation sampling"></a>Diffusion Models and Score distillation sampling</h3><ul>
<li>作者先简单介绍了Diffusion 模型</li>
<li>训练生成模型时，使用加权证据下界（ELBO）进行训练，简化为对参数φ的加权去噪得分匹配目标。训练的过程可以被视为学习一个潜在变量模型，或者学习一系列对应于数据更嘈杂版本的得分函数。</li>
<li>Classifier free Guidance: 使用了无分类器指导（CFG）的方法进行模型的训练，这种方法改变了得分函数，使其更倾向于在条件密度与无条件密度之比较大的区域。这种设置可以在牺牲样本多样性的前提下，提高样本的保真度。</li>
</ul>
<p><img src="/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/Untitled.png" alt="Untitled"></p>
<ul>
<li>实验中，使用了文字描述“a photo of a tree frog wearing a sweater.”（一只穿着毛衣的树蛙的照片）来对文本到图像的扩散模型进行了2D采样方法的比较。</li>
<li>对于得分蒸馏采样，使用了一个图像生成器，这个生成器将图像限制为对称的，具体来说就是使图像的左右两边保持对称，即x = (ﬂip(θ), θ)。</li>
</ul>
<h3 id="HOW-CAN-WE-SAMPLE-IN-PARAMETER-SPACE-NOT-PIXEL-SPACE"><a href="#HOW-CAN-WE-SAMPLE-IN-PARAMETER-SPACE-NOT-PIXEL-SPACE" class="headerlink" title="HOW CAN WE SAMPLE IN PARAMETER SPACE , NOT PIXEL SPACE ?"></a>HOW CAN WE SAMPLE IN PARAMETER SPACE , NOT PIXEL SPACE ?</h3><ul>
<li>Diffusion 模型用在像素空间中建模复杂的分布。作者并不关注像素，目标是生成从随机角度都能渲染出良好的3D模型。</li>
<li>为了创建该模型，作者引入一种可微分图像参数化（DIP）技术，在DIP中，有一个可微分的生成器将一组参数转化为图像。他们将3D体积的参数视为输入参数，而将体积渲染器视为生成器。</li>
<li>原先diffusion 的损失函数并不适用，提出了新的采样方法，叫做得分蒸馏采样（Score Distillation Sampling，SDS）。这种方法使用的是扩散模型的得分函数而不是密度函数。得分函数可以被视为描述模型中数据点如何分布的一种方式。</li>
<li>SDS的主要优点是其效率和鲁棒性。由于扩散模型可以直接预测更新方向，因此不需要通过扩散模型进行反向传播，模型只是预测图像空间的编辑。</li>
</ul>
<p><img src="/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/Untitled%201.png" alt="Untitled"></p>
<h2 id="THE-DREAM-FUSION-ALGORITHM"><a href="#THE-DREAM-FUSION-ALGORITHM" class="headerlink" title="THE DREAM FUSION ALGORITHM"></a>THE DREAM FUSION ALGORITHM</h2><ol>
<li>初始阶段：首先利用Imagen模型（一种预训练模型）并使用NeRF技术初始化一个随机权重的3D模型。</li>
<li>渲染阶段：接着，他们从不同的摄像机位置和角度随机渲染这个3D模型（NeRF）的视图。</li>
<li>更新阶段：这些渲染视图被用作输入，被送入一个得分提取损失函数，该函数环绕Imagen模型。然后，他们使用简单的梯度下降法更新NeRF MLP参数，使得3D模型逐步呈现出与输入文本描述相似的形状和特征。</li>
<li>阴影处理：在模型的渲染过程中，他们还对表面颜色进行了参数化，并对其进行了照明控制（即阴影处理）</li>
<li>后期优化：为了避免模型产生退化解，他们随机将表面颜色替换为白色以产生无纹理的阴影输出。同时，他们也采用了环境图和一个定向损失函数等手段，进一步优化了生成的3D模型。</li>
</ol>
<h3 id="text-to-3d-sythesis"><a href="#text-to-3d-sythesis" class="headerlink" title="text to 3d sythesis"></a>text to 3d sythesis</h3><ol>
<li><p>随机采样相机和光源：在每次迭代中，首先在球形坐标系中随机选取一个相机位置，然后在该位置处生成一个相机姿态矩阵。他们还随机选取一个焦距倍数和一个位于相机位置周围的点光源位置。这一步骤中，各种不同的相机位置和距离的采样对于生成连贯的3D场景和改善学习场景的分辨率是至关重要的。</p>
</li>
<li><p>渲染：给定相机姿态和光源位置，他们将会按照之前描述的方式以64×64的分辨率渲染被照亮的NeRF模型。他们会随机选择是渲染出被照亮的颜色图，还是无纹理的图，或者是不包含任何阴影的纹理图。</p>
</li>
<li><p>视图依赖的条件扩散损失：由于文本提示通常描述的是物体的典型视图，在采样不同视图时可能并不是好的描述。因此，他们发现基于随机采样的相机位置附加视图依赖的文本到给定的输入文本上是有益的。使用了一个预训练的64×64 text2img 模型，并通过如下公式计算NeRF参数的梯度。</p>
<p> <img src="/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/Untitled%202.png" alt="Untitled"></p>
</li>
<li><p>优化：3D场景在TPUv4机器上进行优化，每个芯片渲染一个独立的视图并评估扩散U-Net，每个设备的批大小为1。他们对参数进行了15000次迭代的优化，总共耗时大约1.5小时。他们使用了分布式Shampoo优化器来优化参数。</p>
</li>
</ol>
<p><img src="/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/Untitled%203.png" alt="Untitled"></p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/Untitled%204.png" alt="Untitled"></p>
<p><img src="/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/Untitled%205.png" alt="Untitled"></p>
<ul>
<li>在对象为中心的COCO数据集上，作者使用CLIP L/14来评估我们的未照明渲染（unlit renderings）在泊松反射率（albedo）、全阴影和照明渲染以及无纹理照明几何形状方面的各个组件。</li>
<li>给出了每个消融步骤对于“一只斗牛犬戴着黑色海盗帽”这个提示在泊松反射率（顶部）、阴影（中部）和无纹理渲染（底部）的影响的可视化图。</li>
</ul>
<ol>
<li>基础方法（i）没有视图依赖的提示，结果是一个具有平面几何形状的多面狗。</li>
<li>添加视图依赖提示（ii）改善了几何形状，但表面非常不平滑，导致阴影渲染效果差。</li>
<li>引入照明（iii）改善了几何形状，但暗区（例如帽子）仍然不平滑。</li>
<li>无色渲染（iv）有助于平滑几何形状，但也导致一些颜色细节，如骷髅和交叉骨头被“雕刻”到几何形状中。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/03/23/DreamFusion%20Text-to-3D%20using%202D%20Diffusion%20c53dbb2cf5b94254b0bd32733b75085b/" data-id="clktabfn10008fbrea40vd3ok" data-title="DreamFusion Text-to-3D using 2D Diffusion" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Autoregressive Image Generation using Residual Qua 84f50243875546808d6940440389fbe9" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/04/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/" class="article-date">
  <time class="dt-published" datetime="2023-03-04T02:23:04.000Z" itemprop="datePublished">2023-03-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/03/04/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/">Autoregressive Image Generation using Residual Quantization</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Autoregressive-Image-Generation-using-Residual-Quantization"><a href="#Autoregressive-Image-Generation-using-Residual-Quantization" class="headerlink" title="Autoregressive Image Generation using Residual Quantization"></a>Autoregressive Image Generation using Residual Quantization</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>对于高分辨率图像的自回归建模，VQ方案将图像表示成离散的编码序列。短序列长度对于AR模型是非常重要，它可以减少计算开销, 因为long-range interactions of codes需要被捕捉以便于更好地预测下一个值。然而传统的VQ方案无法在减短code 序列长度的同时生成保真度搞的图像。本文方法包括两个阶段：RQ-VAE /RQ-Transformer. 固定住codebook 大小，RQ-VAE 能够精确地逼近图像的特征图，并将图像表示为离散编码的堆叠图。接着，RQ模型学会预测下一个stack 的编码来预测下一个位置的量化特征向量。由于RQ-VAE将尺寸从256压缩到8，从而也降低了RQ-Transformer的计算量。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled.png" alt="Untitled"></p>
<h3 id="Stage1-Residue-Quantized-VAE"><a href="#Stage1-Residue-Quantized-VAE" class="headerlink" title="Stage1: Residue-Quantized VAE"></a>Stage1: Residue-Quantized VAE</h3><ul>
<li><p>Formulation of VQ and VQ-VAE</p>
<ul>
<li>VQ-VAE执行的是有损压缩，难免会涉及到尺寸H，W和保存信息之间的tradeoff. 如果VQ-VAE的codebook Size 是K，长度为H<em>W</em>Log2k bit. 根据 rate-distortion theory，如果要把H，W图像压缩至 H/2，W/2, 码本大小应该为 $K^4$ 才能保持重建质量。</li>
<li>其他背景知识可以参考 Codeformer, VAE , 不再赘述</li>
</ul>
<p>  <img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled%201.png" alt="Untitled">
  </p>
</li>
<li><p>Residual Quantization</p>
<ul>
<li>用RQ的方式来编码向量z，将向量z 表示为有序的深度为d的编码。注意：这里RQ的编码和Codebook 的编码不是一个东西；RQ 编码的每一个码kd，对应向量z在对应深度下的残差</li>
<li>RQ具体的计算流程如下，RQ采用递归 / coarse-to-fine 的方式来逼近向量z。</li>
</ul>
<p>  $$<br>  \begin{aligned}<br>  \text{RQ}(z; C, D) &amp;= (k_1, \ldots, k_D) \in [K]^D \<br>  k_d &amp;= \text{RQ}(r_{d-1}; C, d),\ \text{for}\ d=1, \ldots, D \<br>  r_d &amp;= r_{d-1} - e(k_d),\ \text{for}\ d=1, \ldots, D \<br>  z^{\hat{ }}<em>d &amp;= \sum</em>{i=1}^d e(k_i),\ \text{for}\ d=1, \ldots, D \<br>  z^{\hat{ }} &amp;= z^{\hat{ }}_D<br>  \end{aligned}<br>  $$</p>
<ul>
<li>这里对于每一个深度，共享同一个codebook。如果codebook 对每个深度都是独立的，那么每个深度的codebook size会不同，会导致更加复杂的超参数搜索。当RQ和VQ codebook相同时，由于RQ存在残差编码，每个深度的空间都是D，因此聚类的数目可以是 $K^D$RQ</li>
</ul>
</li>
<li><p>RQ-VAE</p>
<p>  <img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>loss 则是熟悉的重构loss + commitment loss。 sg代表stop gradient 操作</li>
</ul>
<p>  <img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled%203.png" alt="Untitled"></p>
</li>
</ul>
<h3 id="Stage2-RQ-Transformer"><a href="#Stage2-RQ-Transformer" class="headerlink" title="Stage2: RQ-Transformer"></a>Stage2: RQ-Transformer</h3><ul>
<li><p>AR Modeling for Codes with Depth D</p>
<ul>
<li>RQ-VAE提取出codemap $M \in [K]^{H \times W \times D}$, 然后 reshape 成一个二维数组 $S \in [K]^{T \times D}$, 取它的第t行表示为 $St = (St_1, \dots, St_D) \in [K]^D，t \in [T]$，每行长度为d。</li>
</ul>
<p>  <img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled%204.png" alt="Untitled"></p>
<ul>
<li>RQ Transformer 包括Spatial Transformer 和 Depth Transformer，Spatial Transformer 的输入可以表示为<br>  $u_t = \text{PET}(t) + \sum_{d=1}^D e(St_{t-1,d}) + X$</li>
</ul>
<p>  <img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled%205.png" alt="Untitled"></p>
<ul>
<li><p>Depth Transformer</p>
<p>  $v_{td} = \text{PED}(d) + X + \sum_{d_0=1}^{d-1} e(St_{t,d_0})$</p>
<p>  $L_{AR} = \text{ESE}<em>{t,d}[-\log p(St</em>{t,d} | S_{&lt;t,d}, St_{&lt;d})]$<br>  we</p>
</li>
<li><p>为了解决exposure bias问题，提出了  soft labeling 和 stochastic sampling<br>  of codes from RQ-VAE 两种方法。exposure bias 指的是训练和推理时序列生成过程中产生的偏差。训练时先前的符号都是来自GT，而推理时错误可能会被不断累积。softlabel指的是 NLL损失在这里使用 $Q_t(\cdot | rt,d-1)$作为 $S_{td}$ 的监督信号; stochastic sampling指的是从$Q_t(\cdot | rt,d-1)$采样一个编码作为残差的编码，可以增加泛化性。</p>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li>做了Unconditioned 和 Conditioned 图像生成的比较</li>
</ul>
</li>
</ul>
<p><img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled%206.png" alt="Untitled"></p>
<p><img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled%207.png" alt="Untitled"></p>
<ul>
<li>对比VQ-GAN 和 RQ-Transformer 的采样速度，在batch size = 100 / 200 条件下有着四到五倍多的加速比。</li>
</ul>
<p><img src="/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/Untitled%208.png" alt="Untitled"></p>
<ul>
<li>Ablation<ul>
<li>提升depth比提升codebook的size更加有效</li>
<li>通过depth share codebook机制提升了codebook的利用率</li>
</ul>
</li>
</ul>
<h2 id="Thought"><a href="#Thought" class="headerlink" title="Thought"></a>Thought</h2><p>Depth 的引入，用recursive的形式来表示表示codebook，降低冗余程度，压缩密度更高</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/03/04/Autoregressive%20Image%20Generation%20using%20Residual%20Qua%2084f50243875546808d6940440389fbe9/" data-id="clktabfmw0002fbre2az56ims" data-title="Autoregressive Image Generation using Residual Quantization" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-InstructPix2Pix Learning to Follow Image Editing I 50a3f447c26346a7b22fff99756d70f5" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/02/17/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/" class="article-date">
  <time class="dt-published" datetime="2023-02-17T02:23:04.000Z" itemprop="datePublished">2023-02-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/02/17/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/">InstructPix2Pix</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="InstructPix2Pix-Learning-to-Follow-Image-Editing-Instructions"><a href="#InstructPix2Pix-Learning-to-Follow-Image-Editing-Instructions" class="headerlink" title="InstructPix2Pix: Learning to Follow Image Editing Instructions"></a>InstructPix2Pix: Learning to Follow Image Editing Instructions</h1><h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><p>本文提出的方法：在human instructions 和 给定图像，在指令指示下进行图像编辑。为了获取这个任务的训练数据，利用了GPT3（语言模型）和 Stable Diffusion （text2image）两个预训练模型的能力。Instruct pix2pix是 conditional stable diffusion. 由于无需对模型进行finetune 因此可以快速inference.</p>
<p><img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled.png" alt="Untitled"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li>image editing 的训练还是看成一个 supervised learning 的问题。(1)  生成image editing instruction 和图像  （editing before / after ）构建训练集 （2）用这个训练集训练 diffusion网络</li>
</ul>
<p><img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled%201.png" alt="Untitled"></p>
<ul>
<li><p>Training Data Generation</p>
<ul>
<li>a. 用GPT3 生成编辑的Caption</li>
<li>b. 用Stable Diffusion&amp; Prompt2Prompt 生成图像pair。Prompt2Prompt: 该方法旨在通过在一定数量的去噪步骤中借用交叉注意力权重，使得文本到图像扩散模型中的多个生成物相似。Prompt2Prompt 中有一个参数可以控制两个图像之间的相似度，作者认为仅仅根据标题和编辑文字确定这个值是困难。本文利用clip-based metric 来衡量两个图像之间的consistency，先随机生成100个图像，然后根据这个指标进行筛选。该度量标准衡量了图像对之间的变化在CLIP空间中与标题之间的变化的一致性，这有助于最大化图像对的多样性和质量</li>
<li>c. 生成图像example</li>
</ul>
<p>  <img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled%202.png" alt="Untitled">
  </p>
</li>
<li><p><strong>InstructPix2Pix</strong></p>
<ul>
<li>这里采用了一种改进的Diffusion Model ——  Latent Diffusion, 它通过在pre-trained VAE 的latent space 中进行操作来提高扩散模型的数量和质量。</li>
<li>latent diffusion 核心思想：将图像x 编码为潜在的latent 向量z, 然后在一系列的时间t 中逐渐增加噪声，产生潜在噪声变量z_t；为了预测在给定图像条件cI和文本指令条件cT下添加到噪声潜在变量zt的噪声，我们需要学习一个网络εθ。</li>
<li>这个模型的目标是最小化潜在扩散目标（Latent Diffusion Objective），这个目标计算了网络εθ预测的噪声（εθ(zt, t, E(cI), cT)）与真实噪声ε之间的欧几里得距离（L2范数）的平方。</li>
</ul>
</li>
<li><p>Classifier-free Guidance for two conditioning</p>
</li>
</ul>
<p><img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled%203.png" alt="Untitled"></p>
<ul>
<li>classifier-free diffusion guidance 常用于生成样本中质量和多样性之间的tradeoff。该方法常用于类条件和文本条件图像生成</li>
<li>classifier-free 核心思想：本质是调整概率分布，将原始的概率分布进行转移，使得它更偏向于那些条件 c 高概率的数据。</li>
<li>训练时需要同时训练 「带条件」和「非条件」的去噪扩散模型。在推理时，则将两个预测分数进行合并。通过引导比例 s 和给定条件 c 来调整原始的概率分布，从而使得生成的数据样本更偏向于那些与条件 c 紧密相关的样本。</li>
</ul>
<p><img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled%204.png" alt="Untitled"></p>
<ul>
<li>在训练中，随机将5%的图像置为空，随机将5%的文本置为空，也同时把5%的文本和图像同时置为空。本人无的score network 包括两个条件，因此也要引入两个condition scale。</li>
</ul>
<p><img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled%205.png" alt="Untitled"></p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled%206.png" alt="Untitled"></p>
<h3 id="Baseline-Comparison"><a href="#Baseline-Comparison" class="headerlink" title="Baseline Comparison"></a>Baseline Comparison</h3><ul>
<li>SDEdit 和 Text2Live上做测评。text2live，指在文字prompt的提示下， 做透明和颜色的增广。从图中可以看出，这个问题难点是在保护ID和孤立物体，特别是当需要比较大范围的变化时。</li>
<li>direction similarity: 衡量图像和文本变化一致性度量。方法是先获得两两text embed, 两两图像embed 之间的变化方向，然后计算他们的cosine距离。值也是越高越好。</li>
<li>clip image similarity 跟 direction similarity之间是相互tradeoff的。</li>
</ul>
<p><img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled%207.png" alt="Untitled"></p>
<p><img src="/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/Untitled%208.png" alt="Untitled"></p>
<h3 id="Ablation"><a href="#Ablation" class="headerlink" title="Ablation"></a>Ablation</h3><ul>
<li>当减少数据集数量，模型做large edit 的能力显著降低（更大范围的变化），模型仅仅会在风格上做微小的调整。在指标上就是，图像的一致性很高，但是direction 相似度很低。</li>
<li>Clip filtering 去掉后，输出图像和输入图像的一致性显著降低。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/17/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%2050a3f447c26346a7b22fff99756d70f5/" data-id="clktabfn9000mfbre0ygx4omd" data-title="InstructPix2Pix" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next »</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/04/BLIP2/">BLIP-2 Bootstrapping Language-Image Pre-trainingwith Frozen Image Encoders and Large Language Models</a>
          </li>
        
          <li>
            <a href="/2023/07/02/BLIP/">BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>
          </li>
        
          <li>
            <a href="/2023/07/01/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/">SDXL</a>
          </li>
        
          <li>
            <a href="/2023/06/25/Segment%20Anything/">SAM</a>
          </li>
        
          <li>
            <a href="/2023/05/17/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/">ViLT-Vision-and-Language Transformer</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      © 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>

<script type="text/javascript" charset="utf-8" src="/js/lazyload-plugin/lazyload.intersectionObserver.min.js"></script></body></html>