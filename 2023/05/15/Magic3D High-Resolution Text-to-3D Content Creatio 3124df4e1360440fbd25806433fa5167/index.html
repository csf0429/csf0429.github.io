<!DOCTYPE html><html><head>
  <meta charset="utf-8">
  
  
  <title>Magic3D-High-Resolution Text-to-3D Content Creation | Daniel Cai</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Magic3D: High-Resolution Text-to-3D Content CreationTL;DRDreamFusion 证明了用预训练的文生图diffusion 模型来优化nerf的能力，能够取得较好的text-3d 的结果。该方法存在两方面缺点（a）优化Nerf 速度慢 （b）Nerf上低分辨率优化，会生成低质量的3D模型。本文提出了两阶段方法。（1）使用低分辨率的diffus">
<meta property="og:type" content="article">
<meta property="og:title" content="Magic3D-High-Resolution Text-to-3D Content Creation">
<meta property="og:url" content="http://example.com/2023/05/15/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/index.html">
<meta property="og:site_name" content="Daniel Cai">
<meta property="og:description" content="Magic3D: High-Resolution Text-to-3D Content CreationTL;DRDreamFusion 证明了用预训练的文生图diffusion 模型来优化nerf的能力，能够取得较好的text-3d 的结果。该方法存在两方面缺点（a）优化Nerf 速度慢 （b）Nerf上低分辨率优化，会生成低质量的3D模型。本文提出了两阶段方法。（1）使用低分辨率的diffus">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled.png">
<meta property="og:image" content="http://example.com/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%201.png">
<meta property="og:image" content="http://example.com/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%202.png">
<meta property="og:image" content="http://example.com/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%203.png">
<meta property="og:image" content="http://example.com/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%204.png">
<meta property="og:image" content="http://example.com/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%205.png">
<meta property="article:published_time" content="2023-05-15T02:23:04.000Z">
<meta property="article:modified_time" content="2023-08-01T16:16:42.615Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled.png">
  
    <link rel="alternate" href="/atom.xml" title="Daniel Cai" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Daniel Cai</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Share my paper reading about AIGC &amp; Multimodal &amp; LLM</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"></button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Magic3D High-Resolution Text-to-3D Content Creatio 3124df4e1360440fbd25806433fa5167" class="h-entry article article-type-post" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/05/15/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/" class="article-date">
  <time class="dt-published" datetime="2023-05-15T02:23:04.000Z" itemprop="datePublished">2023-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Magic3D-High-Resolution Text-to-3D Content Creation
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Magic3D-High-Resolution-Text-to-3D-Content-Creation"><a href="#Magic3D-High-Resolution-Text-to-3D-Content-Creation" class="headerlink" title="Magic3D: High-Resolution Text-to-3D Content Creation"></a>Magic3D: High-Resolution Text-to-3D Content Creation</h1><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><p>DreamFusion 证明了用预训练的文生图diffusion 模型来优化nerf的能力，能够取得较好的text-3d 的结果。该方法存在两方面缺点（a）优化Nerf 速度慢 （b）Nerf上低分辨率优化，会生成低质量的3D模型。本文提出了两阶段方法。（1）使用低分辨率的diffusion prior来获得一个“粗糙”的3D Model, 同时用3D 稀疏哈希网格来进行加速。 将这个“粗糙”的表示作为初始化，接着优化一个带纹理的3D mesh model, 优化使用了和高分辨率latent diffusion 相互交互的可微分。该方法能够生成高质量3D hash model，在40分钟之内，相比Dreamfusion 方法有两倍加速，同时给用户提供了控制3D合成的方法。</p>
<p><img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled.png" alt="Untitled"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ul>
<li>DreamFusion 模型包括两个关键部分：（1）场景模型（2）与训练好的text2img 的diffusion 模型。（a）场景模型是一个参数函数x = g(θ)，可以在所需的相机姿态下产生图像x。这里，g是选择的体积渲染器，θ是一个坐标为基础的多层感知机(MLP)，代表一个3D体积。(b)扩散模型ϕ配备了一个已学习的去噪函数ϵ ϕ (x t ; y, t)，可以预测给定噪声图像x t、噪声级别t和文本嵌入y时采样的噪声ϵ。它提供了更新θ的梯度方向，使得所有渲染的图像在扩散先验下，都被推向由文本嵌入条件的高概率密度区域。<br>DreamFusion引入了得分蒸馏采样(Score Distillation Sampling, SDS)，这个过程计算梯度：</li>
</ul>
<p>$$</p>
<p>\nabla_{\theta} L_{SDS} \left( {\phi}, g(\theta) \right) = E_{t, \epsilon} \left[ w(t) \left( \epsilon_{\phi} (x_{t} ; y, t) - \epsilon \right)\frac{\partial x}{\partial \theta}  \right]</p>
<p>$$</p>
<p>去噪函数ϵ ϕ通常被另一个使用无分类器指导（classifier-free guidance）的替代，这允许控制文本条件的强度</p>
<ul>
<li>场景模型选择了带有显式阴影模型的Mip-NeRF 360的变种，并且使用ImageGen 作为扩散模型。这种模型只能在64x64分辨率上操作，无法获得高分辨率的纹理；另外，在大型全局MLP上进行体积渲染在计算和内存上都十分昂贵</li>
</ul>
<h3 id="High-Resolution-3D-Generation"><a href="#High-Resolution-3D-Generation" class="headerlink" title="High-Resolution 3D Generation"></a>High-Resolution 3D Generation</h3><p><img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%201.png" alt="Untitled"></p>
<ul>
<li><p>Coarse-to-fine Diffusion</p>
<ul>
<li>第一阶段，使用了和Dream fusion 中类似的diffusion 模型，该diffusion 先验视用来计算场景模型的梯度，梯度更新依赖于低分辨率低rendered images</li>
<li>第二阶段：使用latent diffusion 在512x512上进行推理，这里使用了stable diffusion model。增加的计算时间主要来自于 $\partial x / \partial \theta$ (来自于高分辨率渲染图像的梯度)， $\partial z / \partial x$ （encoder 的梯度）</li>
</ul>
</li>
<li><p>Scene Models</p>
<ul>
<li>粗糙优化阶段 ： 选择了Instant NGP 中的hash grid encoding ，允许使用更低的计算成本表示高频细节。使用hash grid 和两个单层网络，一个预测反照率和密度，一个预测法线。使用来自Instant NGP的基于密度的体素修剪方法和基于八叉树的射线采样和渲染算法</li>
<li>精细优化阶段：使用带纹理的3D网格来作为精调阶段的场景表示。与为神经场进行体积渲染不同，对带纹理的网格进行可微分栅格化渲染可以在非常高的分辨率下有效地执行。使用coarse 阶段的神经场作为网格几何值的初始值。这里使用可变形的四面体网格（$V_t$，T）来表示3D形状，其中$V_t$是网格T中的顶点。每个顶点$v_i$ ∈ $V_t$ ⊂ R 3 包含一个有符号距离场（SDF）值 $s_i$  ∈ R 和一个顶点从其初始规范坐标的变形 ∆$v_i$ ∈ R 3。然后，使用可微分的行走四面体算法从SDF中提取一个表面网格。对于纹理，使用神经颜色场作为体积纹理的representation。</li>
</ul>
</li>
<li><p>Coarse-to-fine Optimzation</p>
<ul>
<li>Neural field optimization: 初始化一个占用率为256^3的网格（空间中哪些部分会被模型占用），模型周期性地更新这个网格，并使用一种技术叫做八叉树来跳过空的部分，以减少需要处理的计算量。</li>
<li>MLP预测法线：不需要通过计算密度差来估计法线，这可以节省大量的计算资源</li>
<li>使用环境映射MLP模拟背景：为了更好地理解模型应该如何放置在3D场景中，建立一个背景模型。这个模型是用一个小的MLP来创建的，它可以预测光线方向对应的颜色。</li>
<li>网格优化：用一种叫做SDF（有符号距离场）的技术来表示的模型，然后使用一个叫做可微分光栅器的工具将我们的模型渲染成高分辨率的图像。再用高分辨率图像重新回去调整模型</li>
<li>增加焦距来展示细节</li>
<li>使用不同的抗锯齿技术来合成前景和背景</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>  <img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>去除掉背景之后，从图中可以看出Magic3D 方法纹理细节多了很多</li>
</ul>
<p>  <img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%203.png" alt="Untitled"></p>
<p>  <img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%204.png" alt="Untitled"></p>
<ul>
<li><p>Controllable 3D generation</p>
<ul>
<li>Personalized text to 3D:  代表性方法是dreambooth， 首先使用一些特定的图像（例如一只猫/ 一只狗的图片）来微调模型，然后在文本提示中加入一个特殊的标识（例如[V]），就可以根据文本提示来生成一个特定的3D模型。</li>
<li>Prompt-based editing through fine-tuning：分为三个步骤：首先，使用一个prompt来训练一个coarse的模型；然后,改变prompt并使用LDM来对模型进行微调，这个步骤可以提供一个好的初始模型；最后，使用修改后的prompt来优化模型。这种方法在保持模型的基本结构的同时，改变模型的纹理或者形状。</li>
</ul>
<p>  <img src="/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/Untitled%205.png" alt="Untitled"></p>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/05/15/Magic3D%20High-Resolution%20Text-to-3D%20Content%20Creatio%203124df4e1360440fbd25806433fa5167/" data-id="clksjbjwm0004yqlk4yxi0tox" data-title="Magic3D-High-Resolution Text-to-3D Content Creation" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/05/17/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          ViLT-Vision-and-Language Transformer
        
      </div>
    </a>
  
  
    <a href="/2023/04/17/Prompt-to-Prompt%20Image%20Editing%20with%20Cross%20Attentio%20ca603140b6e54d76a5249f2ede70466c/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Prompt-to-Prompt Image Editing</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/04/BLIP2/">BLIP-2 Bootstrapping Language-Image Pre-trainingwith Frozen Image Encoders and Large Language Models</a>
          </li>
        
          <li>
            <a href="/2023/07/02/BLIP/">BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>
          </li>
        
          <li>
            <a href="/2023/07/01/SDXL%20Improving%20Latent%20Diffusion%20Models%20for%20High-Re%20b0dfed5065764baaa371b5b7fd7275cd/">SDXL</a>
          </li>
        
          <li>
            <a href="/2023/06/25/Segment%20Anything/">SAM</a>
          </li>
        
          <li>
            <a href="/2023/05/17/ViLT%20Vision-and-Language%20Transformer%20Without%20Convo%20ddf0a31cddb9402397b4e99fe718307a/">ViLT-Vision-and-Language Transformer</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      © 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>

<script type="text/javascript" charset="utf-8" src="/js/lazyload-plugin/lazyload.intersectionObserver.min.js"></script></body></html>